"""
Langchain MCP Resource Client
Verwendet MCP-Ressourcen direkt als Kontext f√ºr den LLM Agent, ohne k√ºnstliche Tools.
"""

import asyncio
import logging

import httpx
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from langchain_mcp_adapters.client import MultiServerMCPClient
from langchain_ollama import ChatOllama

# Konfiguriere Logging
LOGGER = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


class LangchainMCPResourceClient:
    """
    Ein LangChain-basierter MCP Resource Client, der Ressourcen direkt
    als Kontext f√ºr den Agent verwendet.
    """

    def __init__(self, model_name: str = "llama3.1:8b"):
        """Initialisiere den MCP Resource Client."""
        self.logger = LOGGER
        self.model_name = model_name
        self.llm = None
        self.mcp_client: MultiServerMCPClient | None = None
        self.agent_executor: AgentExecutor | None = None
        self.resource_context = ""

    async def initialize_mcp_client(self) -> None:
        """Initialisiere die MCP-Client-Verbindung."""
        try:
            self.logger.info("Initialisiere MCP-Client...")

            # Konfiguration f√ºr MultiServerMCPClient
            server_config = {
                "default": {
                    "url": "http://127.0.0.1:8000/sse",
                    "transport": "sse",
                },
            }

            self.mcp_client = MultiServerMCPClient(server_config)
            self.logger.info("MCP-Client erfolgreich initialisiert")
        except Exception as e:
            self.logger.error("Fehler beim Initialisieren des MCP-Clients: %s", e)
            raise

    async def check_server_connection(self) -> bool:
        """Pr√ºfe die Verbindung zum MCP-Server."""
        try:
            async with httpx.AsyncClient() as client:
                await client.get("http://127.0.0.1:8000", timeout=5.0)
                return True  # Server antwortet, auch wenn 404
        except Exception as e:
            self.logger.error("Server-Verbindung fehlgeschlagen: %s", e)
            return False

    async def load_resource_context(self) -> None:
        """Lade alle MCP-Ressourcen als Kontext f√ºr den Agent."""
        try:
            # Hole verf√ºgbare Ressourcen √ºber MultiServerMCPClient
            resources = await self.mcp_client.get_resources("default")

            self.logger.info("Verf√ºgbare MCP-Ressourcen: %s", [blob.source for blob in resources])

            # Sammle alle Ressourceninhalte
            context_parts = ["=== VERF√úGBARE MCP-RESSOURCEN ===\n"]

            for blob in resources:
                try:
                    context_parts.append(f"RESSOURCE: {blob.source}")
                    context_parts.append(
                        f"BESCHREIBUNG: {blob.metadata.get('description', 'Keine Beschreibung')}",
                    )
                    context_parts.append(f"INHALT:\n{blob.as_string()}\n")
                    context_parts.append("-" * 50)
                except Exception as e:
                    context_parts.append(f"FEHLER bei {blob.source}: {e}\n")

            self.resource_context = "\n".join(context_parts)
            self.logger.info("Ressourcen-Kontext geladen (%d Zeichen)", len(self.resource_context))

        except Exception as e:
            self.logger.error("Fehler beim Laden der Ressourcen: %s", e)
            self.resource_context = "Keine MCP-Ressourcen verf√ºgbar"

    async def initialize_agent(self) -> None:
        """Initialisiere den Agent mit dem Ressourcen-Kontext."""
        self.logger.info("Initialisiere Agent...")

        # Pr√ºfe Server-Verbindung
        if not await self.check_server_connection():
            raise ConnectionError(
                "Kann nicht zum MCP-Server verbinden. Stelle sicher, dass der Server l√§uft.",
            )

        # Lade Ressourcen-Kontext
        await self.load_resource_context()

        # Initialisiere LLM
        self.llm = ChatOllama(
            model="llama3.2",
            temperature=0.6,
            streaming=False,  # Disable streaming for better compatibility
            base_url="http://host.docker.internal:11434",
        )

        # Erstelle Prompt-Template mit Ressourcen-Kontext
        template = """Du bist ein hilfreicher Assistent mit Zugriff auf MCP-Ressourcen.

VERF√úGBARE RESSOURCEN:
{resource_context}

Du kannst die obigen Ressourcen nutzen, um Fragen zu beantworten. Du hast Zugriff auf:
- README-Informationen des Projekts
- Benutzerdaten 
- Gespr√§chskontext

WERKZEUGE:
Du hast Zugriff auf die folgenden Werkzeuge:

{tools}

Verwende das folgende Format:

Frage: Die Eingabefrage, die du beantworten musst
Gedanke: Du solltest immer √ºber das nachdenken, was zu tun ist
Aktion: Die auszuf√ºhrende Aktion, sollte eine von [{tool_names}] sein
Aktions-Input: Die Eingabe f√ºr die Aktion
Beobachtung: Das Ergebnis der Aktion
... (dieser Gedanke/Aktion/Aktions-Input/Beobachtung kann N-mal wiederholt werden)
Gedanke: Ich kenne jetzt die endg√ºltige Antwort
Endg√ºltige Antwort: Die endg√ºltige Antwort auf die urspr√ºngliche Eingabefrage

Beginne!

Frage: {input}
Gedanke: {agent_scratchpad}"""

        prompt = PromptTemplate.from_template(template)
        # Setze den Ressourcen-Kontext
        prompt = prompt.partial(resource_context=self.resource_context)

        # Erstelle Agent
        agent = create_react_agent(self.llm, prompt)
        self.agent_executor = AgentExecutor(
            agent=agent,
            verbose=True,
            handle_parsing_errors=True,
            max_iterations=3,
        )

        self.logger.info("Agent erfolgreich initialisiert")

    async def chat(self, user_input: str) -> str:
        """Verarbeite eine Benutzereingabe und gib eine Antwort zur√ºck."""
        try:
            result = await self.agent_executor.ainvoke({"input": user_input})
            return result.get("output", "Keine Antwort erhalten")
        except Exception as e:
            self.logger.error("Fehler bei der Chat-Verarbeitung: %s", e)
            return f"Entschuldigung, es gab einen Fehler: {e}"

    async def cleanup(self) -> None:
        """Bereinige die MCP-Client-Verbindung."""
        if self.mcp_client:
            try:
                await self.mcp_client.__aexit__
            except Exception as e:
                self.logger.error("Fehler beim Schlie√üen der MCP-Verbindung: %s", e)

    async def run_interactive_chat(self) -> None:
        """Starte eine interaktive Chat-Session."""
        print("\nü§ñ MCP Resource Chat gestartet!")
        print("üí° Verf√ºgbare Ressourcen wurden geladen und stehen als Kontext zur Verf√ºgung.")
        print(
            "‚ùì Du kannst Fragen zu den Projektdaten, Benutzerdaten oder dem Gespr√§chskontext stellen.",
        )
        print("üö™ Schreibe 'quit' oder 'exit' zum Beenden.\n")

        while True:
            try:
                user_input = input("\nüôã Du: ").strip()

                if user_input.lower() in ["quit", "exit", "q"]:
                    print("\nüëã Auf Wiedersehen!")
                    break

                if not user_input:
                    continue

                print("\nü§î Denke nach...")
                response = await self.chat(user_input)
                print(f"\nü§ñ Assistent: {response}")

            except KeyboardInterrupt:
                print("\n\nüëã Session beendet.")
                break
            except Exception as e:
                print(f"\n‚ùå Fehler: {e}")


async def main() -> None:
    """Start the Langchain MCP Resource Client."""
    client = None
    try:
        # Erstelle und initialisiere Client
        client = LangchainMCPResourceClient()

        # Initialisiere MCP-Verbindung
        await client.initialize_mcp_client()

        # Initialisiere Agent
        await client.initialize_agent()

        # Starte interaktive Chat-Session
        await client.run_interactive_chat()

    except Exception as e:
        LOGGER.info("Fehler beim Starten des Clients: %s", e)
        LOGGER.info(f"‚ùå Fehler: {e}")
    finally:
        # Bereinige Verbindungen
        if client:
            await client.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
